exp_name: mk1_pretrain
seed: 1337
algorithms:
  low_precision_layernorm:
    precision: amp_bf16
  gradient_clipping:
    clipping_type: norm
    clip_norm: 0.25

model:
  _target_: ex_micro_diffusion.models.model.create_latent_diffusion
  vae_name: stabilityai/stable-diffusion-xl-base-1.0
  text_encoder_name: null #openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378
  dit_arch: MicroDiT_Tiny # change this for local expt
  latents_precomputed: true
  in_channels: 4
  pos_interp_scale: 1.0
  dtype: 'bfloat16'
  latent_res: 32
  p_mean: -0.6
  p_std: 1.2
  train_mask_ratio: 0.75

dataset:
  image_size: 256 # 8xlatent res
  train_batch_size: 16
  eval_batch_size: 8
  cap_drop_prob: 0.1
  train:
    _target_: ex_micro_diffusion.datasets.latents_loader.build_streaming_latents_dataloader
    datadir:
      - ./datadir/jdb/jdb_mds_latents/train/
    drop_last: true
    shuffle: true
    prefetch_factor: 2
    num_workers: 2
    persistent_workers: true
    pin_memory: true
  eval:
    _target_: ex_micro_diffusion.datasets.latents_loader.build_streaming_latents_dataloader
    datadir: ./datadir/jdb/jdb_mds_latents/valid # we arent running evals for pretraining eitherway
    drop_last: false
    shuffle: true
    prefetch_factor: 2
    num_workers: 2
    persistent_workers: 2
    pin_memory: true

optimizer:
  _target_: torch.optim.AdamW
  lr: 2.4e-4 # decay to 8e-5 after 256x256 masked pre-training
  weight_decay: 0.1
  eps: 1e-8
  betas:
    - 0.9
    - 0.999

scheduler:
  _target_: composer.optim.CosineAnnealingWithWarmupScheduler
  t_warmup: 250ba # TODO: change
  alpha_f: 0.33 

logger:
  progress:
    _target_: composer.loggers.TensorboardLogger

callbacks:
  speed_monitor:
    _target_: composer.callbacks.speed_monitor.SpeedMonitor
    window_size: 3
  lr_monitor:
    _target_: composer.callbacks.lr_monitor.LRMonitor
  runtime_estimator:
    _target_: composer.callbacks.runtime_estimator.RuntimeEstimator
  optimizer_monitor:
    _target_: composer.callbacks.OptimizerMonitor
  image_monitor:
    _target_: ex_micro_diffusion.models.callbacks.LogDiffusionImages
    # provide a .pt file to load latent prompts directly from if openclip doesnt fit on VRAM with the model
    caption_latents_path: "./utility_clip/caption_latents.pt"
    latent_prompts: null
    prompts:
      - portrait of a beautiful lady with purple hair. Green vines and leaves surrounding her
      - Ocean wave, aesthetic, soothing, turquoise blue
      - Funny portrait of a koala
      - Mysterious tree surrounded by lake
      - A heroic girl's silhouette with moon as backdrop, moon is partially covered by clouds, its evening
      - An armed hero with cape standing near explosion, warm color scheme
      - Malevolent wizard with his staff and hat
      - Watercolor splash art of a butterfly, purple yellow green blue
      - A bird on branch with moon backdrop, ominous, scary 
      - Sunset over mountains, river of blood
      - monster in ominous forest, lake, moon, night
      - volcano galaxy explosion blue purple ominous
    guidance_scale: 5
    sampling_steps: 30
    seed: ${seed}
  nan_catcher:
    _target_: ex_micro_diffusion.models.callbacks.NaNCatcher

trainer:
  _target_: composer.Trainer
  device: gpu
  max_duration: 25ba
  eval_interval: 25ba
  save_interval: 25ba
  save_num_checkpoints_to_keep: 1
  device_train_microbatch_size: 8
  run_name: ${exp_name}
  seed: ${seed}
  save_folder: ./trained_models/${exp_name}/
  save_overwrite: true
  autoresume: false
  fsdp_config:
    sharding_strategy: "SHARD_GRAD_OP"

misc:
  compile: true


